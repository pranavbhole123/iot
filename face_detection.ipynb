{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download Haar Cascade if not present\n",
    "haar_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "haar_path = \"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "if not os.path.exists(haar_path):\n",
    "    print(\"Downloading Haar Cascade...\")\n",
    "    urllib.request.urlretrieve(haar_url, haar_path)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Initialize the Haar Cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(haar_path)\n",
    "\n",
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)  # 0 for default camera\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60, 60),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # Draw rectangles around faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the models\n",
    "FACE_DETECTION_MODEL = \"models/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "FACE_DETECTION_PROTO = \"models/deploy.prototxt\"\n",
    "FACE_EMBEDDING_MODEL = \"models/openface_nn4.small2.v1.t7\"\n",
    "\n",
    "# Thresholds\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Initialize the face detector\n",
    "face_net = cv2.dnn.readNetFromCaffe(FACE_DETECTION_PROTO, FACE_DETECTION_MODEL)\n",
    "\n",
    "# Initialize the face embedding model\n",
    "embed_net = cv2.dnn.readNetFromTorch(FACE_EMBEDDING_MODEL)\n",
    "\n",
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "def extract_features():\n",
    "    user_name = input(\"Enter the name of the authorized user: \")\n",
    "    dataset_path = f\"dataset/{user_name}\"\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    TOTAL_IMAGES = 20\n",
    "\n",
    "    while count < TOTAL_IMAGES:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Resize frame to have a width of 600 pixels for faster processing\n",
    "        frame = cv2.resize(frame, (600, int(frame.shape[0] * 600 / frame.shape[1])))\n",
    "\n",
    "        # Convert the frame to a blob for face detection\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "                                     (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        face_net.setInput(blob)\n",
    "        detections = face_net.forward()\n",
    "\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            if confidence < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Ensure the bounding boxes fall within the dimensions of the frame\n",
    "            startX = max(0, startX)\n",
    "            startY = max(0, startY)\n",
    "            endX = min(w, endX)\n",
    "            endY = min(h, endY)\n",
    "\n",
    "            # Extract the face ROI\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "\n",
    "            # Ensure the face is large enough\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "\n",
    "            # Construct a blob for the face ROI, then pass it through the embedding model\n",
    "            face_blob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                                              (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            embed_net.setInput(face_blob)\n",
    "            vec = embed_net.forward()\n",
    "\n",
    "            # Save the face image and its embedding\n",
    "            face_path = os.path.join(dataset_path, f\"img_{count}.jpg\")\n",
    "            cv2.imwrite(face_path, face)\n",
    "            np.save(os.path.join(dataset_path, f\"img_{count}.npy\"), vec)\n",
    "\n",
    "            print(f\"Captured image {count+1}/{TOTAL_IMAGES}\")\n",
    "            count += 1\n",
    "\n",
    "            # Draw a rectangle around the face and display\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                          (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Image {count}/{TOTAL_IMAGES}\", (startX, startY - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            if count >= TOTAL_IMAGES:\n",
    "                break\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Feature Extraction - Press Q to Quit', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
